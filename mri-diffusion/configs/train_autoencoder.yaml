imports:
  - "$import functools"
  - "$import glob"
  - "$import scripts" # replace with own local path
bundle_root: "."
device: "$torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
ckpt_dir: "$@bundle_root + '/models'"
tf_dir: "$@bundle_root + '/eval'"

# ____________________________ General ____________________________
test: false
offline: "$ True if test else False"
version_name: "v1"
project_name: "3d_vae_gan_perceptual"
dataset_dir: "/workspace/data/medical"
train_batch_size: 2
acc_grad_batches: 4
precision: 32
max_epochs: 1500
train_patch_size:
  - 112
  - 128
  - 80
channel: 0
spacing:
  - 1.1
  - 1.1
  - 1.1
spatial_dims: 3
image_channels: 1
latent_channels: 8

# ---------------------------- Generator Network ----------------------------
generator:
  _target_: "monai.networks.nets.autoencoderkl.AutoencoderKL"
  spatial_dims: "@spatial_dims"
  in_channels: "@image_channels"
  out_channels: "@image_channels"
  latent_channels: "@latent_channels"
  channels:
    - 64
    - 128
    - 256
  num_res_blocks: 2
  norm_num_groups: 32
  norm_eps: 1e-06
  attention_levels:
    - false
    - false
    - false
  with_encoder_nonlocal_attn: false
  with_decoder_nonlocal_attn: false
  include_fc: false

# ---------------------------- Discriminator Network ----------------------------
discriminator:
  _target_: "monai.networks.nets.patchgan_discriminator.PatchDiscriminator"
  spatial_dims: "@spatial_dims"
  num_layers_d: 3
  channels: 32
  in_channels: 1
  out_channels: 1
  norm: "INSTANCE"
# ---------------------------- Optimizers ----------------------------
g_optimizer:
  _target_: "torch.optim.Adam"
  params: "@gnetwork.parameters()"
  lr: 1e-05
  betas:
    - 0.9
    - 0.999
  weight_decay: 0.0

d_optimizer:
    _target_: "torch.optim.Adam"
    params: "@dnetwork.parameters()"
    lr: 1e-05
    betas:
      - 0.9
      - 0.999
    weight_decay: 0.0

# not implemented yet. so only works with d_train_steps: 1
d_train_steps: 1

# ---------------------------- Losses ----------------------------

recon_loss:
  _target_: "torch.nn.L1Loss"
  reduction: "mean"

adv_loss:
    _target_: "generative.losses.adversarial_loss.AdversarialLoss"
    criterion: "least_squares"

kl_loss:
    _target_: "generative.losses.KLLoss3D"
    reduction: "mean"

perceptual_loss:
  _target_: "generative.losses.perceptual.PerceptualLoss"
  spatial_dims: "@spatial_dims"
  network_type: "resnet50"
  is_fake_3d: true
  fake_3d_ratio: 0.2
  pretrained: false
  pretrained_path: null
  pretrained_state_dict_key: "state_dict"

recon_weight: 1.0
adv_weight: 0.1
p_weight: 0.1
kl_weight: 1e-07

# ---------------------------- Lightning Module ----------------------------

LightningModule_def:
  _target_: "models.lightningModules.Lightning3DVaeGanPerceptual"
  g_network: "@generator"
  g_optimizer: "@g_optimizer"
  d_network: "@discriminator"
  d_optimizer: "@d_optimizer"
  recon_loss: "@recon_loss"
  adv_loss: "@adv_loss"
  perceptual_loss: "@perceptual_loss"
  kl_loss: "@kl_loss"
  d_train_steps: "@d_train_steps"
  recon_weight: "@recon_weight"
  adv_weight: "@adv_weight"
  p_weight: "@p_weight"
  kl_weight: "@kl_weight"

# ---------------------------- Preprocessing ----------------------------
# preprocessing is done preliminary in the dataset

# ---------------------------- Data Loader ----------------------------
data_module:
  _target_: "DataLoader"
  dataset: "@dataset"
  batch_size: "@train_batch_size"
  shuffle: true
  num_workers: 0

# ---------------------------- Wandb logger ----------------------------
wandb_logger:
  _target_: "pytorch_lightning.loggers.WandbLogger"
  name: "@version_name"
  project: "@project_name"
  log_model: false
  save_dir: "@tf_dir"

  

# ---------------------------- Callbacks ----------------------------
plotting_callback:
  _target_: "wandblogging.callbacks.PlottingCallback"

model_checkpoint_callback:
  _target_: "pytorch_lightning.callbacks.ModelCheckpoint"
  monitor: "g_loss_val"
  mode: "min"
  save_top_k: 3
  dirpath: "@ckpt_dir"
  filename: "model_min_val_loss-epoch-{epoch:02d}-loss-{g_loss_val:.2f}"
  save_last: True
  every_n_val_epochs: 1

callbacks:
  - "@plotting_callback"
  - "@model_checkpoint_callback"

# ---------------------------- Trainer ----------------------------
#count number of GPUs
num_gpus: "$torch.cuda.device_count()"
#set strategy ddp if more than 1 GPU is available

trainer:
  _target_: "pytorch_lightning.Trainer"
  max_epochs: "@max_epochs"
  logger: "@wandb_logger"
  callbacks: "@callbacks"
  gpus: -1
  strategy: "$ 'ddp' if num_gpus > 1 else None"
  auto_select_gpus: true
  accumulate_grad_batches: "@acc_grad_batches"
  precision: "@precision"
  profiler: "simple"

# ---------------------------- Run ----------------------------
initialize:
  - "$monai.utils.set_determinism(seed=0)"

run:
  - "$@trainer.run(@LightningModule_def, datamodule=@data_module)"

